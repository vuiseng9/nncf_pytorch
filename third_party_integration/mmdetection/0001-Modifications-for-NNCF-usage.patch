From f75bec2a9b6bc5772e55648c5855decf2f0b57c0 Mon Sep 17 00:00:00 2001
From: Aleksei Kashapov <aleksei.kashapov@intel.com>
Date: Mon, 14 Dec 2020 20:41:54 +0300
Subject: [PATCH] Modifications for NNCF usage

---
 .../mask_rcnn_r50_caffe_fpn_1x_coco_int8.py   |  63 +++++++++++
 configs/pascal_voc/ssd300_voc_int8.py         | 103 +++++++++++++++++
 .../retinanet/retinanet_r50_fpn_1x_int8.py    |  49 ++++++++
 mmdet/apis/train.py                           |  29 ++++-
 mmdet/core/__init__.py                        |   1 +
 mmdet/core/export/pytorch2onnx.py             |   8 ++
 mmdet/core/nncf/__init__.py                   |   9 ++
 mmdet/core/nncf/hooks.py                      |  24 ++++
 mmdet/core/nncf/utils.py                      | 105 ++++++++++++++++++
 mmdet/models/dense_heads/anchor_head.py       |   1 +
 mmdet/models/dense_heads/base_dense_head.py   |   5 +-
 mmdet/models/detectors/base.py                |  17 ++-
 .../roi_heads/mask_heads/fcn_mask_head.py     |  13 ++-
 mmdet/models/roi_heads/standard_roi_head.py   |   8 +-
 tools/pytorch2onnx.py                         |  24 ++--
 tools/test.py                                 |  20 +++-
 tools/train.py                                |   6 +
 17 files changed, 454 insertions(+), 31 deletions(-)
 create mode 100644 configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
 create mode 100644 configs/pascal_voc/ssd300_voc_int8.py
 create mode 100644 configs/retinanet/retinanet_r50_fpn_1x_int8.py
 create mode 100644 mmdet/core/nncf/__init__.py
 create mode 100644 mmdet/core/nncf/hooks.py
 create mode 100644 mmdet/core/nncf/utils.py

diff --git a/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py b/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
new file mode 100644
index 0000000..a0e77fe
--- /dev/null
+++ b/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco_int8.py
@@ -0,0 +1,63 @@
+_base_ = './mask_rcnn_r50_fpn_1x_coco.py'
+model = dict(
+    pretrained='open-mmlab://detectron2/resnet50_caffe',
+    backbone=dict(norm_cfg=dict(requires_grad=False), style='caffe'))
+# use caffe img_norm
+img_norm_cfg = dict(
+    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
+train_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
+    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='Pad', size_divisor=32),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(1333, 800),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=True),
+            dict(type='RandomFlip'),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='Pad', size_divisor=32),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    train=dict(pipeline=train_pipeline),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+
+# overload some training params:
+optimizer = dict(type='SGD', lr=0.002, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(grad_clip=None)
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.001,
+    step=[3, 5])
+total_epochs = 6
+work_dir = './work_dirs/mask_rcnn_r50_caffe_fpn_1x_coco_int8'
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth
+load_from = './mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'
+
+find_unused_parameters=True
+
+# nncf config:
+input_size = (1333, 800)
+nncf_config = dict(input_info=dict(sample_size=[1,3,input_size[0],input_size[1]]),
+                   compression=[dict(algorithm="quantization",
+                                     weights=dict(per_channel=True),
+                                     initializer=dict(range=dict(num_init_samples=5,
+                                                                 type='min_max')))],
+                   log_dir=work_dir)
diff --git a/configs/pascal_voc/ssd300_voc_int8.py b/configs/pascal_voc/ssd300_voc_int8.py
new file mode 100644
index 0000000..a3bcaab
--- /dev/null
+++ b/configs/pascal_voc/ssd300_voc_int8.py
@@ -0,0 +1,103 @@
+_base_ = [
+    '../_base_/models/ssd300.py', '../_base_/datasets/voc0712.py',
+    '../_base_/default_runtime.py'
+]
+model = dict(
+    bbox_head=dict(
+        num_classes=20, anchor_generator=dict(basesize_ratio_range=(0.2,
+                                                                    0.9))))
+
+test_cfg = dict(
+    nms=dict(type='nms', iou_threshold=0.45, split_thr=10000000000),
+    min_bbox_size=0,
+    score_thr=0.02,
+    max_per_img=200)
+
+# dataset settings
+dataset_type = 'VOCDataset'
+img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
+train_pipeline = [
+    dict(type='LoadImageFromFile', to_float32=True),
+    dict(type='LoadAnnotations', with_bbox=True),
+    dict(
+        type='PhotoMetricDistortion',
+        brightness_delta=32,
+        contrast_range=(0.5, 1.5),
+        saturation_range=(0.5, 1.5),
+        hue_delta=18),
+    dict(
+        type='Expand',
+        mean=img_norm_cfg['mean'],
+        to_rgb=img_norm_cfg['to_rgb'],
+        ratio_range=(1, 4)),
+    dict(
+        type='MinIoURandomCrop',
+        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
+        min_crop_size=0.3),
+    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
+    dict(type='Normalize', **img_norm_cfg),
+    dict(type='RandomFlip', flip_ratio=0.5),
+    dict(type='DefaultFormatBundle'),
+    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
+]
+test_pipeline = [
+    dict(type='LoadImageFromFile'),
+    dict(
+        type='MultiScaleFlipAug',
+        img_scale=(300, 300),
+        flip=False,
+        transforms=[
+            dict(type='Resize', keep_ratio=False),
+            dict(type='Normalize', **img_norm_cfg),
+            dict(type='ImageToTensor', keys=['img']),
+            dict(type='Collect', keys=['img']),
+        ])
+]
+data = dict(
+    samples_per_gpu=4,
+    workers_per_gpu=4,
+    train=dict(
+        type='RepeatDataset', times=10, dataset=dict(pipeline=train_pipeline)),
+    val=dict(pipeline=test_pipeline),
+    test=dict(pipeline=test_pipeline))
+# optimizer
+optimizer = dict(type='SGD', lr=1e-4, momentum=0.9, weight_decay=5e-4)
+optimizer_config = dict()
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=0.1,
+    step=[16, 20])
+checkpoint_config = dict(interval=1)
+
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+
+# runtime settings
+total_epochs = 24
+
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/ssd300_voc_int8'
+workflow = [('train', 1)]
+
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://s3.ap-northeast-2.amazonaws.com/open-mmlab/mmdetection/models/ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth
+load_from = './ssd300_voc_vgg16_caffe_240e_20190501-7160d09a.pth'
+resume_from = None
+
+# nncf config
+input_size = (300, 300)
+nncf_config = dict(input_info=dict(sample_size=[1,3,input_size[0],input_size[1]])
+                   ,compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_samples=10,
+                                                                 type="threesigma")))],
+                   log_dir=work_dir)
diff --git a/configs/retinanet/retinanet_r50_fpn_1x_int8.py b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
new file mode 100644
index 0000000..60ae180
--- /dev/null
+++ b/configs/retinanet/retinanet_r50_fpn_1x_int8.py
@@ -0,0 +1,49 @@
+_base_ = [
+    '../_base_/models/retinanet_r50_fpn.py',
+    '../_base_/datasets/coco_detection.py',
+    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
+]
+
+test_cfg = dict(
+    nms_pre=1000,
+    min_bbox_size=0,
+    score_thr=0.05,
+    nms=dict(type='nms', iou_threshold=0.5, split_thr=10000000000),
+    max_per_img=100)
+
+# optimizer
+optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
+optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
+
+# learning policy
+lr_config = dict(
+    policy='step',
+    warmup='linear',
+    warmup_iters=500,
+    warmup_ratio=1.0 / 10,
+    step=[8, 11])
+checkpoint_config = dict(interval=1)
+# yapf:disable
+log_config = dict(
+    interval=50,
+    hooks=[
+        dict(type='TextLoggerHook'),
+        # dict(type='TensorboardLoggerHook')
+    ])
+# yapf:enable
+# runtime settings
+total_epochs = 3
+dist_params = dict(backend='nccl')
+log_level = 'INFO'
+work_dir = './work_dirs/retinanet_r50_fpn_1x_int8'
+# we start from the pre-trained checkpoint available from the mmdet github repo
+# download from here: https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/models/retinanet_r50_fpn_2x_20190616-75574209.pth
+load_from = './retinanet_r50_fpn_2x_20190616-75574209.pth'
+resume_from = None
+workflow = [('train', 1)]
+# nncf config
+input_size = (1333, 800)
+nncf_config = dict(input_info=dict(sample_size=[1,3,input_size[0],input_size[1]]),
+                   compression=[dict(algorithm='quantization',
+                                     initializer=dict(range=dict(num_init_samples=10)))],
+                   log_dir=work_dir)
diff --git a/mmdet/apis/train.py b/mmdet/apis/train.py
index ad17a53..719e9bc 100644
--- a/mmdet/apis/train.py
+++ b/mmdet/apis/train.py
@@ -4,14 +4,17 @@ import numpy as np
 import torch
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
-                         Fp16OptimizerHook, OptimizerHook, build_optimizer)
+                         Fp16OptimizerHook, OptimizerHook, build_optimizer, load_checkpoint)
 from mmcv.utils import build_from_cfg

-from mmdet.core import DistEvalHook, EvalHook
+from mmdet.core import DistEvalHook, EvalHook, Fp16OptimizerHook, CompressionHook
 from mmdet.datasets import (build_dataloader, build_dataset,
                             replace_ImageToTensor)
 from mmdet.utils import get_root_logger

+from nncf.torch.utils import get_all_modules
+from mmdet.core.nncf import wrap_nncf_model
+

 def set_random_seed(seed, deterministic=False):
     """Set random seed.
@@ -68,13 +71,25 @@ def train_detector(model,
             seed=cfg.seed) for ds in dataset
     ]

+    if cfg.load_from:
+        load_checkpoint(model=model, filename=cfg.load_from)
+
     # put model on gpus
+    model = model.cuda()
+
+    # nncf model wrapper
+    if cfg.ENABLE_COMPRESSION:
+        model, compression_ctrl = wrap_nncf_model(model, cfg, data_loaders[0])
+        print(*get_all_modules(model).keys(), sep="\n")
+    else:
+        compression_ctrl = None
+
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         model = MMDistributedDataParallel(
-            model.cuda(),
+            model,
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
@@ -82,6 +97,9 @@ def train_detector(model,
         model = MMDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)

+    if cfg.ENABLE_COMPRESSION and distributed:
+        compression_ctrl.distributed()
+
     # build runner
     optimizer = build_optimizer(model, cfg.optimizer)
     runner = EpochBasedRunner(
@@ -109,6 +127,8 @@ def train_detector(model,
                                    cfg.get('momentum_config', None))
     if distributed:
         runner.register_hook(DistSamplerSeedHook())
+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))

     # register eval hooks
     if validate:
@@ -143,6 +163,9 @@ def train_detector(model,
             hook = build_from_cfg(hook_cfg, HOOKS)
             runner.register_hook(hook, priority=priority)

+    if cfg.ENABLE_COMPRESSION:
+        runner.register_hook(CompressionHook(compression_ctrl=compression_ctrl))
+
     if cfg.resume_from:
         runner.resume(cfg.resume_from)
     elif cfg.load_from:
diff --git a/mmdet/core/__init__.py b/mmdet/core/__init__.py
index b075369..d8fc903 100644
--- a/mmdet/core/__init__.py
+++ b/mmdet/core/__init__.py
@@ -6,3 +6,4 @@ from .fp16 import *  # noqa: F401, F403
 from .mask import *  # noqa: F401, F403
 from .post_processing import *  # noqa: F401, F403
 from .utils import *  # noqa: F401, F403
+from .nncf import *
diff --git a/mmdet/core/export/pytorch2onnx.py b/mmdet/core/export/pytorch2onnx.py
index 8f9309d..91efe16 100644
--- a/mmdet/core/export/pytorch2onnx.py
+++ b/mmdet/core/export/pytorch2onnx.py
@@ -5,6 +5,7 @@ import numpy as np
 import torch
 from mmcv.runner import load_checkpoint

+from ..nncf.utils import wrap_nncf_model

 def generate_inputs_and_wrap_model(config_path, checkpoint_path, input_config):
     """Prepare sample input and wrap model for ONNX export.
@@ -81,6 +82,13 @@ def build_model_from_cfg(config_path, checkpoint_path):
     # build the model
     model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)
     load_checkpoint(model, checkpoint_path, map_location='cpu')
+
+    # nncf model wrapper
+    if 'nncf_config' in cfg:
+        print('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.nncf_load_from = checkpoint_path
+        model, compression_ctrl = wrap_nncf_model(model, cfg, None)
+
     model.cpu().eval()
     return model

diff --git a/mmdet/core/nncf/__init__.py b/mmdet/core/nncf/__init__.py
new file mode 100644
index 0000000..bc743b9
--- /dev/null
+++ b/mmdet/core/nncf/__init__.py
@@ -0,0 +1,9 @@
+from .hooks import CompressionHook
+from .utils import wrap_nncf_model
+from .utils import load_checkpoint
+
+__all__ = [
+    'CompressionHook',
+    'wrap_nncf_model',
+    'load_checkpoint',
+]
diff --git a/mmdet/core/nncf/hooks.py b/mmdet/core/nncf/hooks.py
new file mode 100644
index 0000000..d6b3887
--- /dev/null
+++ b/mmdet/core/nncf/hooks.py
@@ -0,0 +1,16 @@
+from texttable import Texttable
+from mmcv.runner.hooks.hook import Hook
+
+
+class CompressionHook(Hook):
+    def __init__(self, compression_ctrl=None):
+        self.compression_ctrl = compression_ctrl
+
+    def after_train_iter(self, runner):
+        self.compression_ctrl.scheduler.step()
+
+    def after_train_epoch(self, runner):
+        self.compression_ctrl.scheduler.epoch_step()
+
+    def before_run(self, runner):
+        runner.logger.info(self.compression_ctrl.statistics().to_str())
diff --git a/mmdet/core/nncf/utils.py b/mmdet/core/nncf/utils.py
new file mode 100644
index 0000000..c903a55
--- /dev/null
+++ b/mmdet/core/nncf/utils.py
@@ -0,0 +1,109 @@
+import pathlib
+from collections import OrderedDict
+
+import torch
+from nncf.torch.initialization import PTInitializingDataLoader
+from nncf.config.structures import QuantizationRangeInitArgs
+from nncf.config.structures import BNAdaptationInitArgs
+
+from nncf import NNCFConfig
+from nncf.torch import load_state
+from nncf.torch import create_compressed_model
+from nncf.torch import nncf_model_input
+
+
+def wrap_nncf_model(model, cfg, data_loader_for_init=None):
+    pathlib.Path(cfg.work_dir).mkdir(parents=True, exist_ok=True)
+    nncf_config = NNCFConfig.from_dict(cfg.nncf_config)
+
+    if data_loader_for_init is not None:
+        wrapped_loader = MMInitializeDataLoader(data_loader_for_init)
+
+        nncf_config.register_extra_structs([
+            QuantizationRangeInitArgs(wrapped_loader),
+            BNAdaptationInitArgs(wrapped_loader),
+        ])
+
+    input_size = nncf_config.get(
+        'sample_size', (1, 3, cfg.input_size[0], cfg.input_size[1])
+    )
+
+    def wrap_inputs(args, kwargs):
+        # during model's forward
+        if 'img' in kwargs:
+            img = kwargs['img']
+            if isinstance(img, list):
+                assert len(img) == 1, 'Input list must have a length 1'
+                assert torch.is_tensor(img[0]), 'Input for a model must be a tensor'
+                img[0] = nncf_model_input(img[0])
+            else:
+                assert torch.is_tensor(img), 'Input for a model must be a tensor'
+                img = nncf_model_input(img)
+            kwargs['img'] = img
+            return args, kwargs
+
+        # during model's export
+        if isinstance(args, tuple):
+            assert torch.is_tensor(args[0][0]), 'Input for a model must be a tensor'
+            for img in args[0]:
+                img = nncf_model_input(img)
+
+        return args, kwargs
+
+    def dummy_forward(model):
+        device = next(model.parameters()).device
+        input_args = ([nncf_model_input(torch.randn(input_size).to(device)), ],)
+        input_kwargs = dict(return_loss=False, dummy_forward=True)
+        model(*input_args, **input_kwargs)
+
+    model.dummy_forward_fn = dummy_forward
+
+    checkpoint = None
+    if cfg.get('nncf_load_from', None):
+        checkpoint = load_checkpoint(model, cfg.nncf_load_from)
+
+    compression_ctrl, model = create_compressed_model(
+        model,
+        nncf_config,
+        resuming_state_dict=checkpoint,
+        dummy_forward_fn=dummy_forward,
+        wrap_inputs_fn=wrap_inputs,
+    )
+
+    return model, compression_ctrl
+
+
+def load_checkpoint(model, filename, map_location=None, strict=False):
+    """Load checkpoint from a file or URI.
+
+    Args:
+        model (Module): Module to load checkpoint.
+        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.
+        map_location (str): Same as :func:`torch.load`.
+        strict (bool): Whether to allow different params for the model and
+            checkpoint.
+
+    Returns:
+        dict or OrderedDict: The loaded checkpoint.
+    """
+    # load checkpoint from modelzoo or file or url
+    checkpoint = torch.load(filename, map_location=map_location)
+    # get state_dict from checkpoint
+    if isinstance(checkpoint, OrderedDict):
+        state_dict = checkpoint
+    elif isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
+        state_dict = checkpoint['state_dict']
+    else:
+        raise RuntimeError('No state_dict found in checkpoint file {}'.format(filename))
+    _ = load_state(model, state_dict, strict)
+    return checkpoint
+
+
+class MMInitializeDataLoader(PTInitializingDataLoader):
+    def get_inputs(self, dataloader_output):
+        # redefined PTInitializingDataLoader because
+        # of DataContainer format in mmdet
+        kwargs = {k: v.data[0] for k, v in dataloader_output.items()}
+        return (), kwargs
+
+    # get_targets TBD
diff --git a/mmdet/models/dense_heads/anchor_head.py b/mmdet/models/dense_heads/anchor_head.py
index a5bb413..48332a8 100644
--- a/mmdet/models/dense_heads/anchor_head.py
+++ b/mmdet/models/dense_heads/anchor_head.py
@@ -2,6 +2,7 @@ import torch
 import torch.nn as nn
 from mmcv.cnn import normal_init
 from mmcv.runner import force_fp32
+from nncf.torch.dynamic_graph.context import no_nncf_trace

 from mmdet.core import (anchor_inside_flags, build_anchor_generator,
                         build_assigner, build_bbox_coder, build_sampler,
diff --git a/mmdet/models/dense_heads/base_dense_head.py b/mmdet/models/dense_heads/base_dense_head.py
index de11e4a..0fcdc4d 100644
--- a/mmdet/models/dense_heads/base_dense_head.py
+++ b/mmdet/models/dense_heads/base_dense_head.py
@@ -1,6 +1,7 @@
 from abc import ABCMeta, abstractmethod

 import torch.nn as nn
+from nncf.torch.dynamic_graph.context import no_nncf_trace


 class BaseDenseHead(nn.Module, metaclass=ABCMeta):
@@ -51,7 +52,9 @@ class BaseDenseHead(nn.Module, metaclass=ABCMeta):
             loss_inputs = outs + (gt_bboxes, img_metas)
         else:
             loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
-        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
+
+        with no_nncf_trace():
+            losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
         if proposal_cfg is None:
             return losses
         else:
diff --git a/mmdet/models/detectors/base.py b/mmdet/models/detectors/base.py
index 7c6d5e9..488650e 100644
--- a/mmdet/models/detectors/base.py
+++ b/mmdet/models/detectors/base.py
@@ -135,6 +135,9 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             if not isinstance(var, list):
                 raise TypeError(f'{name} must be a list, but got {type(var)}')

+        if 'dummy_forward' in kwargs:
+            return self.forward_dummy(imgs[0])
+
         num_augs = len(imgs)
         if num_augs != len(img_metas):
             raise ValueError(f'num of augmentations ({len(imgs)}) '
@@ -165,8 +168,12 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
             assert 'proposals' not in kwargs
             return self.aug_test(imgs, img_metas, **kwargs)

-    @auto_fp16(apply_to=('img', ))
-    def forward(self, img, img_metas, return_loss=True, **kwargs):
+    @abstractmethod
+    def forward_dummy(self, img, **kwargs):
+        pass
+
+    @auto_fp16(apply_to=('img',))
+    def forward(self, img, img_metas=[], return_loss=True, **kwargs):
         """Calls either :func:`forward_train` or :func:`forward_test` depending
         on whether ``return_loss`` is ``True``.

@@ -216,7 +223,7 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):

         return loss, log_vars

-    def train_step(self, data, optimizer):
+    def train_step(self, data, optimizer, compression_ctrl=None):
         """The iteration step during training.

         This method defines an iteration step during training, except for the
@@ -246,6 +253,10 @@ class BaseDetector(nn.Module, metaclass=ABCMeta):
         losses = self(**data)
         loss, log_vars = self._parse_losses(losses)

+        if compression_ctrl is not None:
+            compression_loss = compression_ctrl.loss()
+            loss += compression_loss
+
         outputs = dict(
             loss=loss, log_vars=log_vars, num_samples=len(data['img_metas']))

diff --git a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
index 0cba3cd..d41b158 100644
--- a/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
+++ b/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py
@@ -10,6 +10,8 @@ from torch.nn.modules.utils import _pair
 from mmdet.core import mask_target
 from mmdet.models.builder import HEADS, build_loss

+from nncf.torch.dynamic_graph.context import no_nncf_trace
+
 BYTES_PER_FLOAT = 4
 # TODO: This memory limit may be too much or too little. It would be better to
 # determine it based on available resources.
@@ -140,11 +142,12 @@ class FCNMaskHead(nn.Module):
         if mask_pred.size(0) == 0:
             loss_mask = mask_pred.sum()
         else:
-            if self.class_agnostic:
-                loss_mask = self.loss_mask(mask_pred, mask_targets,
-                                           torch.zeros_like(labels))
-            else:
-                loss_mask = self.loss_mask(mask_pred, mask_targets, labels)
+            with no_nncf_trace():
+                if self.class_agnostic:
+                    loss_mask = self.loss_mask(mask_pred, mask_targets,
+                                               torch.zeros_like(labels))
+                else:
+                    loss_mask = self.loss_mask(mask_pred, mask_targets, labels)
         loss['loss_mask'] = loss_mask
         return loss

diff --git a/mmdet/models/roi_heads/standard_roi_head.py b/mmdet/models/roi_heads/standard_roi_head.py
index c530f2a..30d98a4 100644
--- a/mmdet/models/roi_heads/standard_roi_head.py
+++ b/mmdet/models/roi_heads/standard_roi_head.py
@@ -1,4 +1,5 @@
 import torch
+from nncf.torch.dynamic_graph.context import no_nncf_trace

 from mmdet.core import bbox2result, bbox2roi, build_assigner, build_sampler
 from ..builder import HEADS, build_head, build_roi_extractor
@@ -151,9 +152,10 @@ class StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin):

         bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,
                                                   gt_labels, self.train_cfg)
-        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
-                                        bbox_results['bbox_pred'], rois,
-                                        *bbox_targets)
+        with no_nncf_trace():
+            loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],
+                                            bbox_results['bbox_pred'], rois,
+                                            *bbox_targets)

         bbox_results.update(loss_bbox=loss_bbox)
         return bbox_results
diff --git a/tools/pytorch2onnx.py b/tools/pytorch2onnx.py
index a8e7487..41f59c4 100644
--- a/tools/pytorch2onnx.py
+++ b/tools/pytorch2onnx.py
@@ -39,17 +39,19 @@ def pytorch2onnx(config_path,
     if model.with_mask:
         output_names.append('masks')

-    torch.onnx.export(
-        model,
-        tensor_data,
-        output_file,
-        input_names=['input'],
-        output_names=output_names,
-        export_params=True,
-        keep_initializers_as_inputs=True,
-        do_constant_folding=True,
-        verbose=show,
-        opset_version=opset_version)
+    with torch.no_grad():
+        torch.onnx.export(
+            model,
+            tensor_data,
+            output_file,
+            input_names=['input'],
+            output_names=output_names,
+            export_params=True,
+            keep_initializers_as_inputs=True,
+            do_constant_folding=True,
+            verbose=show,
+            opset_version=opset_version,
+            enable_onnx_checker=False)

     model.forward = orig_model.forward
     print(f'Successfully exported ONNX model: {output_file}')
diff --git a/tools/test.py b/tools/test.py
index 8dcd305..0333d30 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -15,6 +15,8 @@ from mmdet.datasets import (build_dataloader, build_dataset,
                             replace_ImageToTensor)
 from mmdet.models import build_detector

+from mmdet.core.nncf import wrap_nncf_model
+

 def parse_args():
     parser = argparse.ArgumentParser(
@@ -160,11 +162,19 @@ def main():
     # build the model and load checkpoint
     model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)
     fp16_cfg = cfg.get('fp16', None)
-    if fp16_cfg is not None:
-        wrap_fp16_model(model)
-    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
-    if args.fuse_conv_bn:
-        model = fuse_conv_bn(model)
+    # nncf model wrapper
+    if 'nncf_config' in cfg:
+        cfg.nncf_load_from = args.checkpoint
+        model.cuda()  # for wrap_nncf_model
+        model, compression_ctrl = wrap_nncf_model(model, cfg, None)
+        checkpoint = torch.load(args.checkpoint, map_location=None)
+    else:
+        if fp16_cfg is not None:
+            wrap_fp16_model(model)
+        checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
+        if args.fuse_conv_bn:
+            model = fuse_conv_bn(model)
+
     # old versions did not save class info in checkpoints, this walkaround is
     # for backward compatibility
     if 'CLASSES' in checkpoint['meta']:
diff --git a/tools/train.py b/tools/train.py
index 91a59b8..458ad26 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -142,6 +142,12 @@ def main():
     logger.info(f'Distributed training: {distributed}')
     logger.info(f'Config:\n{cfg.pretty_text}')
 
+    if 'nncf_config' in cfg:
+        logger.info('NNCF config: {}'.format(cfg.nncf_config))
+        cfg.ENABLE_COMPRESSION = True
+    else:
+        cfg.ENABLE_COMPRESSION = False
+
     # set random seeds
     if args.seed is not None:
         logger.info(f'Set random seed to {args.seed}, '
-- 
2.28.0.windows.1

